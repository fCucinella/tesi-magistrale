{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from keras.models import load_model\n",
    "\n",
    "from adversarialdefence.mutation_testing import AdversarialDetectorThroughMutation\n",
    "from adversarialdefence.model_mut_operators import ModelMutationOperators\n",
    "\n",
    "from adversarialdefence.utils import ModelUtils, GeneralUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Data Loading***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1715326, 49)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../csv/CICIDS2017_improved-preprocessed.csv')\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of benign samples: 1432918\n",
      "Number of anomalous samples: 282408\n"
     ]
    }
   ],
   "source": [
    "df_benign = df[df['Label'] == 0]\n",
    "df_anomalous = df[df['Label'] == 1]\n",
    "\n",
    "print(f'Number of benign samples: {df_benign.shape[0]}')\n",
    "print(f'Number of anomalous samples: {df_anomalous.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Load models and scalers***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "autoencoder = load_model('../modelli/autoencoder_best_weights_96-96.hdf5')\n",
    "dnn_retrained = load_model('../modelli/dnn_retrained_best_weights.hdf5')\n",
    "dnn = load_model('../modelli/DNN_best_weights_99.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.2.2 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "std_scaler_aut = joblib.load('../modelli/std_scaler_aut.bin')\n",
    "std_scaler_dnn = joblib.load('../modelli/std_scaler_dnn.bin')\n",
    "columns = df.copy().drop('Label', axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Define detectors***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUTATED_MODELS_BASE_PATH = '../modelli/mutation/'\n",
    "adv_detector_dnn = AdversarialDetectorThroughMutation(dnn, 'DNN', 'SUPERVISED', 0.5, MUTATED_MODELS_BASE_PATH)\n",
    "adv_detector_aut = AdversarialDetectorThroughMutation(autoencoder, 'AUT', 'UNSUPERVISED', 0.05, MUTATED_MODELS_BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_benign = df_benign.sample(n=1000)\n",
    "y_benign = X_benign.pop('Label')\n",
    "\n",
    "X_benign_dnn = std_scaler_dnn.transform(X_benign)\n",
    "X_benign_aut = std_scaler_aut.transform(X_benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "LCR_th_dnn = adv_detector_dnn.fit(X_benign_dnn, 500, 0.3)\n",
    "LCR_th_aut = adv_detector_dnn.fit(X_benign_aut, 500, 0.006)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Load BB Adversarial samples***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_ADVS_CSV_PATH = '../csv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hsj, X_hsj, y_hsj = GeneralUtils.get_data_with_advs(BASE_ADVS_CSV_PATH + 'hsj_new.csv')\n",
    "df_zoo, X_zoo, y_zoo = GeneralUtils.get_data_with_advs(BASE_ADVS_CSV_PATH + 'zoo_new.csv')\n",
    "df_boundary, X_boundary, y_boundary = GeneralUtils.get_data_with_advs(BASE_ADVS_CSV_PATH + 'boundary_new.csv')\n",
    "df_query_eff, X_query_eff, y_query_eff = GeneralUtils.get_data_with_advs(BASE_ADVS_CSV_PATH + 'query_eff_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_hsj_dnn_retrained = ModelUtils.binary_preds_supervised(dnn_retrained, X_hsj)\n",
    "preds_zoo_dnn_retrained = ModelUtils.binary_preds_supervised(dnn_retrained, X_zoo)\n",
    "preds_boundary_dnn_retrained = ModelUtils.binary_preds_supervised(dnn_retrained, X_boundary)\n",
    "preds_query_eff_dnn_retrained = ModelUtils.binary_preds_supervised(dnn_retrained, X_query_eff)\n",
    "\n",
    "preds_hsj_aut_retrained = ModelUtils.binary_preds_unsupervised(autoencoder, X_hsj, 0.035)\n",
    "preds_zoo_aut_retrained = ModelUtils.binary_preds_unsupervised(autoencoder, X_zoo, 0.035)\n",
    "preds_boundary_aut_retrained = ModelUtils.binary_preds_unsupervised(autoencoder, X_boundary, 0.035)\n",
    "preds_query_eff_aut_retrained = ModelUtils.binary_preds_unsupervised(autoencoder, X_query_eff, 0.035)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "advs_hsj_dnn_retrained = GeneralUtils.get_advs_samples(preds_hsj_dnn_retrained, df_hsj)\n",
    "advs_zoo_dnn_retrained = GeneralUtils.get_advs_samples(preds_zoo_dnn_retrained, df_zoo)\n",
    "advs_boundary_dnn_retrained = GeneralUtils.get_advs_samples(preds_boundary_dnn_retrained, df_boundary)\n",
    "advs_query_eff_dnn_retrained = GeneralUtils.get_advs_samples(preds_query_eff_dnn_retrained, df_query_eff)\n",
    "\n",
    "advs_hsj_aut_retrained = GeneralUtils.get_advs_samples(preds_hsj_aut_retrained, df_hsj)\n",
    "advs_zoo_aut_retrained = GeneralUtils.get_advs_samples(preds_zoo_aut_retrained, df_zoo)\n",
    "advs_boundary_aut_retrained = GeneralUtils.get_advs_samples(preds_boundary_aut_retrained, df_boundary)\n",
    "advs_query_eff_aut_retrained = GeneralUtils.get_advs_samples(preds_query_eff_aut_retrained, df_query_eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_benign = df_benign.sample(n=10000)\n",
    "y_benign = X_benign.pop('Label')\n",
    "\n",
    "X_anomalous = df_anomalous.sample(n=10000)\n",
    "y_anomalous = X_anomalous.pop('Label')\n",
    "\n",
    "X_benign_dnn = std_scaler_dnn.transform(X_benign)\n",
    "X_benign_aut = std_scaler_aut.transform(X_benign)\n",
    "\n",
    "X_anomalous_dnn = std_scaler_dnn.transform(X_anomalous)\n",
    "X_anomalous_aut = std_scaler_aut.transform(X_anomalous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_attacks_dnn_retrained_dict = {\n",
    "    'HOPSKIPJUMP': advs_hsj_dnn_retrained, \n",
    "    'BOUNDARY': advs_boundary_dnn_retrained, \n",
    "    'ZOO': advs_zoo_dnn_retrained, \n",
    "    'QUERY_EFF': advs_query_eff_dnn_retrained\n",
    "}\n",
    "\n",
    "bb_attacks_aut_retrained_dict = {\n",
    "    'HOPSKIPJUMP': advs_hsj_aut_retrained, \n",
    "    'BOUNDARY': advs_boundary_aut_retrained, \n",
    "    'ZOO': advs_zoo_aut_retrained, \n",
    "    'QUERY_EFF': advs_query_eff_aut_retrained\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Performance of Target System (Adversarial Training + Mutation Testing)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCommonAdversarialSamples(X_advs, df_advs):\n",
    "    preds_advs_dnn = ModelUtils.binary_preds_supervised(dnn_retrained, X_advs)\n",
    "    advs_samples_dnn = GeneralUtils.get_advs_samples(np.array(preds_advs_dnn), df_advs)\n",
    "\n",
    "    preds_advs_dnn_by_aut = ModelUtils.binary_preds_unsupervised(autoencoder, advs_samples_dnn.copy().drop('Label', axis=1), 0.035)\n",
    "    return GeneralUtils.get_advs_samples(np.array(preds_advs_dnn_by_aut), advs_samples_dnn)\n",
    "\n",
    "def getFirstLayerFalseBenignSamples(preds_dnn, preds_aut):\n",
    "    X_anomalous_with_preds = X_anomalous.assign(preds_dnn=preds_dnn.astype(bool), preds_aut=np.array(preds_aut).astype(bool))\n",
    "    X_false_benign = X_anomalous_with_preds[(X_anomalous_with_preds['preds_dnn'] == False) & (X_anomalous_with_preds['preds_aut'] == False)]   \n",
    "\n",
    "    X_false_benign = X_false_benign.drop(['preds_dnn', 'preds_aut'], axis=1)\n",
    "    return X_false_benign\n",
    "\n",
    "def getFirstLayerRealBenignSamples(preds_dnn, preds_aut):\n",
    "    X_benign_with_preds = X_benign.assign(preds_dnn=preds_dnn.astype(bool), preds_aut=np.array(preds_aut).astype(bool))\n",
    "    X_real_benign = X_benign_with_preds[(X_benign_with_preds['preds_dnn'] == False) & (X_benign_with_preds['preds_aut'] == False)]   \n",
    "\n",
    "    X_real_benign = X_real_benign.drop(['preds_dnn', 'preds_aut'], axis=1)\n",
    "    return X_real_benign\n",
    "\n",
    "def perform_detection(X, attack_type, n_step, do_scaling=False):\n",
    "    num_detected_list = []\n",
    "    for i in range(0, n_step):\n",
    "        if do_scaling:\n",
    "            detect_status_dnn = adv_detector_dnn.detect(std_scaler_dnn.transform(X), 200, 0.009)\n",
    "            #detect_status_aut = adv_detector_aut.detect(std_scaler_aut.transform(X), 200, 0.7)\n",
    "        else:\n",
    "            detect_status_dnn = adv_detector_dnn.detect(X, 200, 0.009)\n",
    "            #detect_status_aut = adv_detector_aut.detect(X, 200, 0.7)\n",
    "\n",
    "        #df_detect_status = pd.DataFrame({\n",
    "        #    'dnn': detect_status_dnn,\n",
    "        #    'aut': detect_status_aut\n",
    "        #}).reset_index(drop=True)\n",
    "\n",
    "        num_detected_list.append(len([d for d in detect_status_dnn if d == True]))\n",
    "        #num_detected_list.append(df_detect_status[(df_detect_status['dnn'] == True) | (df_detect_status['aut'] == True)].shape[0])\n",
    "    \n",
    "    avg_detected = round(sum(num_detected_list) / len(num_detected_list))\n",
    "    print(f'Average number of aversarial samples of type {attack_type} detected in {n_step} steps: {avg_detected} / {X.shape[0]} - Ratio {(avg_detected/X.shape[0])*100}%') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_advs_hsj_common = getCommonAdversarialSamples(X_hsj, df_hsj)\n",
    "df_advs_boundary_common = getCommonAdversarialSamples(X_boundary, df_boundary)\n",
    "df_advs_zoo_common = getCommonAdversarialSamples(X_zoo, df_zoo)\n",
    "df_advs_query_eff_common = getCommonAdversarialSamples(X_query_eff, df_query_eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of aversarial samples of type HOPSKIPJUMP detected in 1 steps: 2 / 7 - Ratio 28.57142857142857%\n",
      "Average number of aversarial samples of type BOUNDARY detected in 1 steps: 2 / 9 - Ratio 22.22222222222222%\n",
      "Average number of aversarial samples of type ZOO detected in 1 steps: 3 / 10 - Ratio 30.0%\n",
      "Average number of aversarial samples of type QUERY EFF detected in 1 steps: 0 / 7 - Ratio 0.0%\n"
     ]
    }
   ],
   "source": [
    "perform_detection(df_advs_hsj_common.copy().drop('Label',axis=1), 'HOPSKIPJUMP', 1)\n",
    "perform_detection(df_advs_boundary_common.copy().drop('Label',axis=1), 'BOUNDARY', 1)\n",
    "perform_detection(df_advs_zoo_common.copy().drop('Label',axis=1), 'ZOO', 1)\n",
    "perform_detection(df_advs_query_eff_common.copy().drop('Label',axis=1), 'QUERY EFF', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of aversarial samples of type FALSE BENIGN detected in 1 steps: 0 / 17 - Ratio 0.0%\n"
     ]
    }
   ],
   "source": [
    "preds_anomalous_dnn_retrained = ModelUtils.binary_preds_supervised(dnn_retrained, X_anomalous_dnn)\n",
    "preds_anomalous_aut_retrained = ModelUtils.binary_preds_supervised(autoencoder, X_anomalous_aut, 0.035)\n",
    "\n",
    "X_first_layer_false_benign = getFirstLayerFalseBenignSamples(preds_anomalous_dnn_retrained, preds_anomalous_aut_retrained)\n",
    "perform_detection(X_first_layer_false_benign, 'FALSE BENIGN', 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of aversarial samples of type REAL BENIGN detected in 1 steps: 2037 / 9247 - Ratio 22.02876608629826%\n"
     ]
    }
   ],
   "source": [
    "preds_benign_dnn_retrained = ModelUtils.binary_preds_supervised(dnn_retrained, X_benign_dnn)\n",
    "preds_benign_aut_retrained = ModelUtils.binary_preds_unsupervised(autoencoder, X_benign_aut, 0.035)\n",
    "\n",
    "X_first_layer_real_benign = getFirstLayerRealBenignSamples(preds_benign_dnn_retrained, preds_benign_aut_retrained)\n",
    "perform_detection(X_first_layer_real_benign, 'REAL BENIGN', 1, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Attempt 2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "X_benign = df_benign.sample(n=1000)\n",
    "y_benign = X_benign.pop('Label')\n",
    "\n",
    "X_benign_dnn = std_scaler_dnn.transform(X_benign)\n",
    "X_benign_aut = std_scaler_aut.transform(X_benign)\n",
    "\n",
    "LCR_th_dnn = adv_detector_dnn.fit(X_benign_dnn, 500, 0.1)\n",
    "#LCR_th_aut = adv_detector_dnn.fit(X_benign_aut, 500, 0.006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of aversarial samples of type HOPSKIPJUMP detected in 1 steps: 4 / 7 - Ratio 57.14285714285714%\n",
      "Average number of aversarial samples of type BOUNDARY detected in 1 steps: 2 / 9 - Ratio 22.22222222222222%\n",
      "Average number of aversarial samples of type ZOO detected in 1 steps: 2 / 10 - Ratio 20.0%\n",
      "Average number of aversarial samples of type QUERY EFF detected in 1 steps: 0 / 7 - Ratio 0.0%\n"
     ]
    }
   ],
   "source": [
    "perform_detection(df_advs_hsj_common.copy().drop('Label',axis=1), 'HOPSKIPJUMP', 1)\n",
    "perform_detection(df_advs_boundary_common.copy().drop('Label',axis=1), 'BOUNDARY', 1)\n",
    "perform_detection(df_advs_zoo_common.copy().drop('Label',axis=1), 'ZOO', 1)\n",
    "perform_detection(df_advs_query_eff_common.copy().drop('Label',axis=1), 'QUERY EFF', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_benign = df_benign.sample(n=10000)\n",
    "y_benign = X_benign.pop('Label')\n",
    "\n",
    "X_anomalous = df_anomalous.sample(n=100000)\n",
    "y_anomalous = X_anomalous.pop('Label')\n",
    "\n",
    "X_benign_dnn = std_scaler_dnn.transform(X_benign)\n",
    "X_benign_aut = std_scaler_aut.transform(X_benign)\n",
    "\n",
    "X_anomalous_dnn = std_scaler_dnn.transform(X_anomalous)\n",
    "X_anomalous_aut = std_scaler_aut.transform(X_anomalous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of aversarial samples of type FALSE BENIGN detected in 1 steps: 37 / 120 - Ratio 30.833333333333336%\n"
     ]
    }
   ],
   "source": [
    "preds_anomalous_dnn_retrained = ModelUtils.binary_preds_supervised(dnn_retrained, X_anomalous_dnn)\n",
    "preds_anomalous_aut_retrained = ModelUtils.binary_preds_supervised(autoencoder, X_anomalous_aut, 0.035)\n",
    "\n",
    "X_first_layer_false_benign = getFirstLayerFalseBenignSamples(preds_anomalous_dnn_retrained, preds_anomalous_aut_retrained)\n",
    "perform_detection(X_first_layer_false_benign, 'FALSE BENIGN', 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of aversarial samples of type REAL BENIGN detected in 1 steps: 46 / 9193 - Ratio 0.5003807244642663%\n"
     ]
    }
   ],
   "source": [
    "preds_benign_dnn_retrained = ModelUtils.binary_preds_supervised(dnn_retrained, X_benign_dnn)\n",
    "preds_benign_aut_retrained = ModelUtils.binary_preds_unsupervised(autoencoder, X_benign_aut, 0.035)\n",
    "\n",
    "X_first_layer_real_benign = getFirstLayerRealBenignSamples(preds_benign_dnn_retrained, preds_benign_aut_retrained)\n",
    "perform_detection(X_first_layer_real_benign, 'REAL BENIGN', 1, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
