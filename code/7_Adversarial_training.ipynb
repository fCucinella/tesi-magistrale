{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "from adversarialdefence.utils import ModelUtils, GeneralUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Data Loading***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1715326, 49)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../csv/CICIDS2017_improved-preprocessed.csv')\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of benign samples: 1432918\n",
      "Number of anomalous samples: 282408\n"
     ]
    }
   ],
   "source": [
    "df_benign = df[df['Label'] == 0]\n",
    "df_anomalous = df[df['Label'] == 1]\n",
    "\n",
    "print(f'Number of benign samples: {df_benign.shape[0]}')\n",
    "print(f'Number of anomalous samples: {df_anomalous.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Load models and scalers***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "autoencoder = load_model('../modelli/autoencoder_best_weights_96-96.hdf5')\n",
    "dnn = load_model('../modelli/DNN_best_weights_99.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.2.2 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "std_scaler_aut = joblib.load('../modelli/std_scaler_aut.bin')\n",
    "std_scaler_dnn = joblib.load('../modelli/std_scaler_dnn.bin')\n",
    "columns = df.copy().drop('Label', axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Adversarial WB samples loading***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_advs_csv_path = '../csv/'\n",
    "\n",
    "df_fgsm, X_fgsm, y_fgsm = GeneralUtils.get_data_with_advs(base_advs_csv_path + 'fgsm.csv')\n",
    "df_deepFool, X_deepFool, y_deepFool = GeneralUtils.get_data_with_advs(base_advs_csv_path + 'deepFool.csv')\n",
    "df_carlini, X_carlini, y_carlini = GeneralUtils.get_data_with_advs(base_advs_csv_path + 'carlini.csv')\n",
    "\n",
    "X_carlini.drop(X_carlini.columns[0], axis=1, inplace=True)\n",
    "df_carlini.drop(df_carlini.columns[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_fgsm_dnn = ModelUtils.binary_preds_supervised(dnn, X_fgsm)\n",
    "preds_deepFool_dnn = ModelUtils.binary_preds_supervised(dnn, X_deepFool)\n",
    "preds_carlini_dnn = ModelUtils.binary_preds_supervised(dnn, X_carlini)\n",
    "\n",
    "preds_fgsm_aut = ModelUtils.binary_preds_unsupervised(autoencoder, X_fgsm)\n",
    "preds_deepFool_aut = ModelUtils.binary_preds_unsupervised(autoencoder, X_deepFool)\n",
    "preds_carlini_aut = ModelUtils.binary_preds_unsupervised(autoencoder, X_carlini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "advs_fgsm_dnn = GeneralUtils.get_advs_samples(preds_fgsm_dnn, df_fgsm)\n",
    "advs_deepFool_dnn = GeneralUtils.get_advs_samples(preds_deepFool_dnn, df_deepFool)\n",
    "advs_carlini_dnn = GeneralUtils.get_advs_samples(preds_carlini_dnn, df_carlini)\n",
    "\n",
    "advs_fgsm_aut = GeneralUtils.get_advs_samples(preds_fgsm_aut, df_fgsm)\n",
    "advs_deepFool_aut = GeneralUtils.get_advs_samples(preds_deepFool_aut, df_deepFool)\n",
    "advs_carlini_aut = GeneralUtils.get_advs_samples(preds_carlini_aut, df_carlini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of adversarial samples for dnn: 358351\n",
      "Number of adversarial samples for aut: 270813\n"
     ]
    }
   ],
   "source": [
    "df_advs_dnn = pd.concat([advs_fgsm_dnn, advs_deepFool_dnn, advs_carlini_dnn], ignore_index=True).sample(frac=1)\n",
    "df_advs_aut = pd.concat([advs_fgsm_aut, advs_deepFool_aut, advs_carlini_aut], ignore_index=True).sample(frac=1)\n",
    "\n",
    "print(f'Number of adversarial samples for dnn: {df_advs_dnn.shape[0]}')\n",
    "print(f'Number of adversarial samples for aut: {df_advs_aut.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Data splitting***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_splitting_AUT_with_advs(df_benign, df_anomalous, df_advs):\n",
    "    \n",
    "    df_benign = pd.DataFrame(std_scaler_aut.transform(df_benign.sample(frac=0.15).drop('Label', axis=1)), columns=columns).assign(Label=0)\n",
    "    df_anomalous = pd.DataFrame(std_scaler_aut.transform(df_anomalous.copy().drop('Label', axis=1)), columns=columns).assign(Label=1)\n",
    "            \n",
    "    df_test = pd.concat([df_benign, df_anomalous], ignore_index=True).sample(frac=1.0)\n",
    "\n",
    "    print(f'Anomalous samples test AUT: {df_anomalous.shape[0]}')\n",
    "    print(f'Benign samples test AUT: {df_benign.shape[0]}')\n",
    "    print(f'Adversarial samples test AUT: {df_advs.shape[0]}')\n",
    "    print()\n",
    "    \n",
    "    return df_test\n",
    "\n",
    "def data_splitting_DNN_with_advs(df_benign, df_anomalous, df_advs):\n",
    "    df_benign = pd.DataFrame(std_scaler_dnn.transform(df_benign.copy().drop('Label', axis=1)), columns=columns).assign(Label=0)\n",
    "    df_anomalous = pd.DataFrame(std_scaler_dnn.transform(df_anomalous.copy().drop('Label', axis=1)), columns=columns).assign(Label=1)\n",
    "    \n",
    "    df_anomalous_train, df_anomalous_test = train_test_split(df_anomalous, test_size=0.25, random_state=42)\n",
    "    df_anomalous_train, df_anomalous_val = train_test_split(df_anomalous_train, test_size=0.15, random_state=42)\n",
    "\n",
    "    df_benign_train, df_benign_test = train_test_split(df_benign, test_size=0.10, random_state=42)\n",
    "    df_benign_train, df_benign_val = train_test_split(df_benign_train, test_size=0.10, random_state=42)\n",
    "    \n",
    "    df_advs_train, df_advs_test = train_test_split(df_advs, test_size=0.20, random_state=42)\n",
    "    df_advs_train, df_advs_val = train_test_split(df_advs_train, test_size=0.20, random_state=42)\n",
    "\n",
    "    df_benign_train = df_benign_train.sample(frac=0.40)\n",
    "\n",
    "    print(f'Anomalous samples train/val/test DNN: {df_anomalous_train.shape[0]} - {df_anomalous_val.shape[0]} - {df_anomalous_test.shape[0]}')\n",
    "    print(f'Benign samples train/val/test DNN: {df_benign_train.shape[0]} - {df_benign_val.shape[0]} - {df_benign_test.shape[0]}')\n",
    "    print(f'Adversarial samples train/val/test DNN: {df_advs_train.shape[0]} - {df_advs_val.shape[0]} - {df_advs_test.shape[0]}')\n",
    "    print()\n",
    "    \n",
    "    df_train = pd.concat([df_benign_train, df_anomalous_train, df_advs_train], ignore_index=True).sample(frac=1)\n",
    "    df_test = pd.concat([df_benign_test, df_anomalous_test, df_advs_test], ignore_index=True).sample(frac=1)\n",
    "    df_val = pd.concat([df_benign_val, df_anomalous_val, df_advs_val], ignore_index=True).sample(frac=1)\n",
    "    \n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalous samples test AUT: 282408\n",
      "Benign samples test AUT: 214938\n",
      "Adversarial samples test AUT: 270813\n",
      "\n",
      "Anomalous samples train/val/test DNN: 180035 - 31771 - 70602\n",
      "Benign samples train/val/test DNN: 464265 - 128963 - 143292\n",
      "Adversarial samples train/val/test DNN: 229344 - 57336 - 71671\n",
      "\n",
      "Number of train/val/test samples for dnn: 873644 - 218070 - 285565\n",
      "Number of test samples for aut: 497346\n"
     ]
    }
   ],
   "source": [
    "df_test_aut = data_splitting_AUT_with_advs(df_benign, df_anomalous, df_advs_aut)\n",
    "df_train_dnn, df_val_dnn, df_test_dnn = data_splitting_DNN_with_advs(df_benign, df_anomalous, df_advs_dnn)\n",
    "\n",
    "print(f'Number of train/val/test samples for dnn: {df_train_dnn.shape[0]} - {df_val_dnn.shape[0]} - {df_test_dnn.shape[0]}')\n",
    "print(f'Number of test samples for aut: {df_test_aut.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training datasets\n",
    "X_train_dnn = df_train_dnn.copy()\n",
    "y_train_dnn = X_train_dnn.pop('Label')\n",
    "\n",
    "#Validation datasets\n",
    "X_val_dnn = df_val_dnn.copy()\n",
    "y_val_dnn = X_val_dnn.pop('Label')\n",
    "\n",
    "#Test datasets\n",
    "X_test_aut = df_test_aut.copy()\n",
    "y_test_aut = X_test_aut.pop('Label')\n",
    "\n",
    "X_test_dnn = df_test_dnn.copy()\n",
    "y_test_dnn = X_test_dnn.pop('Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Aversarial retraining***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "save_model = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='dnn_retrained_best_weights.hdf5',\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    verbose=0,\n",
    "    mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "854/854 [==============================] - 2s 2ms/step - loss: 0.1264 - acc: 0.9696 - val_loss: 0.0431 - val_acc: 0.9934\n",
      "Epoch 2/10\n",
      "128/854 [===>..........................] - ETA: 0s - loss: 0.0436 - acc: 0.9927"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "854/854 [==============================] - 1s 1ms/step - loss: 0.0413 - acc: 0.9929 - val_loss: 0.0378 - val_acc: 0.9936\n",
      "Epoch 3/10\n",
      "854/854 [==============================] - 1s 1ms/step - loss: 0.0385 - acc: 0.9931 - val_loss: 0.0324 - val_acc: 0.9946\n",
      "Epoch 4/10\n",
      "854/854 [==============================] - 1s 1ms/step - loss: 0.0281 - acc: 0.9953 - val_loss: 0.0240 - val_acc: 0.9962\n",
      "Epoch 5/10\n",
      "854/854 [==============================] - 1s 2ms/step - loss: 0.0251 - acc: 0.9958 - val_loss: 0.0242 - val_acc: 0.9960\n",
      "Epoch 6/10\n",
      "854/854 [==============================] - 1s 1ms/step - loss: 0.0249 - acc: 0.9958 - val_loss: 0.0239 - val_acc: 0.9961\n",
      "Epoch 7/10\n",
      "854/854 [==============================] - 2s 2ms/step - loss: 0.0249 - acc: 0.9958 - val_loss: 0.0236 - val_acc: 0.9961\n",
      "Epoch 8/10\n",
      "854/854 [==============================] - 1s 2ms/step - loss: 0.0249 - acc: 0.9958 - val_loss: 0.0231 - val_acc: 0.9962\n",
      "Epoch 9/10\n",
      "854/854 [==============================] - 1s 1ms/step - loss: 0.0245 - acc: 0.9958 - val_loss: 0.0230 - val_acc: 0.9962\n",
      "Epoch 10/10\n",
      "854/854 [==============================] - 1s 1ms/step - loss: 0.0245 - acc: 0.9958 - val_loss: 0.0231 - val_acc: 0.9962\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 10\n",
    "\n",
    "history = dnn.fit(\n",
    "    X_train_dnn, y_train_dnn,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop, save_model],\n",
    "    validation_data=(X_val_dnn, y_val_dnn)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_retrained = load_model('../modelli/dnn_retrained_best_weights.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Testing on WB Adversarial samples***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_ADVS_CSV_PATH = '../csv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkAdversarialResults(X_advs, df_advs, attack_type, num_total_attempts, dnn_target, aut_binary_th=0.05, context=''):\n",
    "    preds_advs_dnn = ModelUtils.binary_preds_supervised(dnn_target, X_advs)\n",
    "    advs_samples_dnn = GeneralUtils.get_advs_samples(np.array(preds_advs_dnn), df_advs)\n",
    "    print(f'Successful adversarial samples of type {attack_type} for DNN {context}: {advs_samples_dnn.shape[0]} / {num_total_attempts} - ASR: {(advs_samples_dnn.shape[0]/num_total_attempts) * 100}%')\n",
    "\n",
    "    preds_advs_aut = ModelUtils.binary_preds_unsupervised(autoencoder, X_advs, aut_binary_th)\n",
    "    advs_samples_aut = GeneralUtils.get_advs_samples(np.array(preds_advs_aut), df_advs)\n",
    "    print(f'Successful adversarial samples of type {attack_type} for AUT {context}: {advs_samples_aut.shape[0]} / {num_total_attempts} - ASR: {(advs_samples_aut.shape[0]/num_total_attempts) * 100}%')\n",
    "\n",
    "    preds_advs_dnn_by_aut = ModelUtils.binary_preds_unsupervised(autoencoder, advs_samples_dnn.copy().drop('Label', axis=1), aut_binary_th)\n",
    "    common_advs_samples = GeneralUtils.get_advs_samples(np.array(preds_advs_dnn_by_aut), advs_samples_dnn)\n",
    "    print(f'Successful adversarial samples of type {attack_type} for TARGET SYSTEM {context}: {common_advs_samples.shape[0]} / {num_total_attempts} - ATSR: {(common_advs_samples.shape[0]/num_total_attempts) * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FGSM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful adversarial samples of type FGSM for DNN : 94759 / 282408 - ASR: 33.55393614911759%\n",
      "Successful adversarial samples of type FGSM for AUT : 16858 / 282408 - ASR: 5.969377638027251%\n",
      "Successful adversarial samples of type FGSM for TARGET SYSTEM : 4645 / 282408 - ATSR: 1.6447834338970568%\n",
      "---------\n",
      "Successful adversarial samples of type FGSM for DNN RETRAINED: 475 / 282408 - ASR: 0.1681963683748336%\n",
      "Successful adversarial samples of type FGSM for AUT RETRAINED: 1289 / 282408 - ASR: 0.45643182912665364%\n",
      "Successful adversarial samples of type FGSM for TARGET SYSTEM RETRAINED: 2 / 282408 - ATSR: 0.0007081952352624572%\n"
     ]
    }
   ],
   "source": [
    "checkAdversarialResults(X_fgsm, df_fgsm, 'FGSM', df_anomalous.shape[0], dnn)\n",
    "print('---------')\n",
    "checkAdversarialResults(X_fgsm, df_fgsm, 'FGSM', df_anomalous.shape[0], dnn_retrained, 0.035, 'RETRAINED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEEPFOOL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful adversarial samples of type DEEPFOOL for DNN : 238261 / 282408 - ASR: 84.36765247443415%\n",
      "Successful adversarial samples of type DEEPFOOL for AUT : 24430 / 282408 - ASR: 8.650604798730914%\n",
      "Successful adversarial samples of type DEEPFOOL for TARGET SYSTEM : 206 / 282408 - ATSR: 0.0729441092320331%\n",
      "---------\n",
      "Successful adversarial samples of type DEEPFOOL for DNN RETRAINED: 1874 / 282408 - ASR: 0.6635789354409224%\n",
      "Successful adversarial samples of type DEEPFOOL for AUT RETRAINED: 12629 / 282408 - ASR: 4.471898813064786%\n",
      "Successful adversarial samples of type DEEPFOOL for TARGET SYSTEM RETRAINED: 0 / 282408 - ATSR: 0.0%\n"
     ]
    }
   ],
   "source": [
    "checkAdversarialResults(X_deepFool, df_deepFool, 'DEEPFOOL', df_anomalous.shape[0], dnn)\n",
    "print('---------')\n",
    "checkAdversarialResults(X_deepFool, df_deepFool, 'DEEPFOOL', df_anomalous.shape[0], dnn_retrained, 0.035, 'RETRAINED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Carlini**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful adversarial samples of type CARLINI for DNN : 25331 / 282408 - ASR: 8.96964675221665%\n",
      "Successful adversarial samples of type CARLINI for AUT : 229525 / 282408 - ASR: 81.27425568680773%\n",
      "Successful adversarial samples of type CARLINI for TARGET SYSTEM : 11243 / 282408 - ATSR: 3.9811195150279026%\n",
      "---------\n",
      "Successful adversarial samples of type CARLINI for DNN RETRAINED: 467 / 282408 - ASR: 0.16536358743378374%\n",
      "Successful adversarial samples of type CARLINI for AUT RETRAINED: 143168 / 282408 - ASR: 50.69544772102773%\n",
      "Successful adversarial samples of type CARLINI for TARGET SYSTEM RETRAINED: 74 / 282408 - ATSR: 0.026203223704710916%\n"
     ]
    }
   ],
   "source": [
    "checkAdversarialResults(X_carlini, df_carlini, 'CARLINI', df_anomalous.shape[0], dnn)\n",
    "print('---------')\n",
    "checkAdversarialResults(X_carlini, df_carlini, 'CARLINI', df_anomalous.shape[0], dnn_retrained, 0.035, 'RETRAINED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Testing on BB Adversarial samples***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HopSkipJump**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful adversarial samples of type HOPSKIPJUMP for DNN : 49667 / 50000 - ASR: 99.334%\n",
      "Successful adversarial samples of type HOPSKIPJUMP for AUT : 21511 / 50000 - ASR: 43.022%\n",
      "Successful adversarial samples of type HOPSKIPJUMP for TARGET SYSTEM : 21500 / 50000 - ATSR: 43.0%\n",
      "---------\n",
      "Successful adversarial samples of type HOPSKIPJUMP for DNN RETRAINED: 127 / 50000 - ASR: 0.254%\n",
      "Successful adversarial samples of type HOPSKIPJUMP for AUT RETRAINED: 5132 / 50000 - ASR: 10.264%\n",
      "Successful adversarial samples of type HOPSKIPJUMP for TARGET SYSTEM RETRAINED: 7 / 50000 - ATSR: 0.013999999999999999%\n"
     ]
    }
   ],
   "source": [
    "df_hsj, X_hsj, y_hsj = GeneralUtils.get_data_with_advs(BASE_ADVS_CSV_PATH + 'hsj_new.csv')\n",
    "checkAdversarialResults(X_hsj, df_hsj, 'HOPSKIPJUMP', 50000, dnn)\n",
    "print('---------')\n",
    "checkAdversarialResults(X_hsj, df_hsj, 'HOPSKIPJUMP', 50000, dnn_retrained, 0.035, 'RETRAINED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boundary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful adversarial samples of type BOUNDARY for DNN : 49847 / 50000 - ASR: 99.694%\n",
      "Successful adversarial samples of type BOUNDARY for AUT : 191 / 50000 - ASR: 0.382%\n",
      "Successful adversarial samples of type BOUNDARY for TARGET SYSTEM : 191 / 50000 - ATSR: 0.382%\n",
      "---------\n",
      "Successful adversarial samples of type BOUNDARY for DNN RETRAINED: 39597 / 50000 - ASR: 79.194%\n",
      "Successful adversarial samples of type BOUNDARY for AUT RETRAINED: 124 / 50000 - ASR: 0.248%\n",
      "Successful adversarial samples of type BOUNDARY for TARGET SYSTEM RETRAINED: 9 / 50000 - ATSR: 0.018000000000000002%\n"
     ]
    }
   ],
   "source": [
    "df_boundary, X_boundary, y_boundary = GeneralUtils.get_data_with_advs(BASE_ADVS_CSV_PATH + 'boundary_new.csv')\n",
    "checkAdversarialResults(X_boundary, df_boundary, 'BOUNDARY', 50000, dnn)\n",
    "print('---------')\n",
    "checkAdversarialResults(X_boundary, df_boundary, 'BOUNDARY', 50000, dnn_retrained, 0.035, 'RETRAINED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ZOO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful adversarial samples of type ZOO for DNN : 3535 / 50000 - ASR: 7.07%\n",
      "Successful adversarial samples of type ZOO for AUT : 40192 / 50000 - ASR: 80.384%\n",
      "Successful adversarial samples of type ZOO for TARGET SYSTEM : 978 / 50000 - ATSR: 1.9560000000000002%\n",
      "---------\n",
      "Successful adversarial samples of type ZOO for DNN RETRAINED: 80 / 50000 - ASR: 0.16%\n",
      "Successful adversarial samples of type ZOO for AUT RETRAINED: 25028 / 50000 - ASR: 50.056%\n",
      "Successful adversarial samples of type ZOO for TARGET SYSTEM RETRAINED: 10 / 50000 - ATSR: 0.02%\n"
     ]
    }
   ],
   "source": [
    "df_zoo, X_zoo, y_zoo = GeneralUtils.get_data_with_advs(BASE_ADVS_CSV_PATH + 'zoo_new.csv')\n",
    "checkAdversarialResults(X_zoo, df_zoo, 'ZOO', 50000, dnn)\n",
    "print('---------')\n",
    "checkAdversarialResults(X_zoo, df_zoo, 'ZOO', 50000, dnn_retrained, 0.035, 'RETRAINED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query Efficient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful adversarial samples of type QUERY EFF for DNN : 24205 / 50000 - ASR: 48.41%\n",
      "Successful adversarial samples of type QUERY EFF for AUT : 21389 / 50000 - ASR: 42.778%\n",
      "Successful adversarial samples of type QUERY EFF for TARGET SYSTEM : 37 / 50000 - ATSR: 0.074%\n",
      "---------\n",
      "Successful adversarial samples of type QUERY EFF for DNN RETRAINED: 8562 / 50000 - ASR: 17.124%\n",
      "Successful adversarial samples of type QUERY EFF for AUT RETRAINED: 13471 / 50000 - ASR: 26.942%\n",
      "Successful adversarial samples of type QUERY EFF for TARGET SYSTEM RETRAINED: 7 / 50000 - ATSR: 0.013999999999999999%\n"
     ]
    }
   ],
   "source": [
    "df_query_eff_new, X_query_eff_new, y_query_eff_new = GeneralUtils.get_data_with_advs(BASE_ADVS_CSV_PATH + 'query_eff_new.csv')\n",
    "checkAdversarialResults(X_query_eff_new, df_query_eff_new, 'QUERY EFF', 50000, dnn)\n",
    "print('---------')\n",
    "checkAdversarialResults(X_query_eff_new, df_query_eff_new, 'QUERY EFF', 50000, dnn_retrained, 0.035, 'RETRAINED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Test Prediction Error on NON Adversarial samples***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_benign = df_benign.sample(n=100000)\n",
    "y_benign = X_benign.pop('Label')\n",
    "\n",
    "X_anomalous = df_anomalous.sample(n=100000)\n",
    "y_anomalous = X_anomalous.pop('Label')\n",
    "\n",
    "X_benign_dnn = std_scaler_dnn.transform(X_benign)\n",
    "X_benign_aut = std_scaler_aut.transform(X_benign)\n",
    "\n",
    "X_anomalous_dnn = std_scaler_dnn.transform(X_anomalous)\n",
    "X_anomalous_aut = std_scaler_aut.transform(X_anomalous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkFalseAnomalous(preds_dnn, preds_aut):\n",
    "    X_benign_with_preds = X_benign.assign(preds_dnn=preds_dnn.astype(bool), preds_aut=np.array(preds_aut).astype(bool))\n",
    "    X_false_anomalous = X_benign_with_preds[(X_benign_with_preds['preds_dnn'] == True) | (X_benign_with_preds['preds_aut'] == True)]   \n",
    "\n",
    "    print(f'False anomalous: {X_false_anomalous.shape[0]} / {X_benign.shape[0]} - Rate {(X_false_anomalous.shape[0] / X_benign.shape[0]) * 100}%')\n",
    "\n",
    "def checkFalseBenign(preds_dnn, preds_aut):\n",
    "    X_anomalous_with_preds = X_anomalous.assign(preds_dnn=preds_dnn.astype(bool), preds_aut=np.array(preds_aut).astype(bool))\n",
    "    X_false_benign = X_anomalous_with_preds[(X_anomalous_with_preds['preds_dnn'] == False) & (X_anomalous_with_preds['preds_aut'] == False)]   \n",
    "\n",
    "    print(f'False benign: {X_false_benign.shape[0]} / {X_anomalous.shape[0]} - Rate {(X_false_benign.shape[0] / X_anomalous.shape[0]) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non retrained predictions on NON adveersarial samples\n",
    "preds_benign_dnn = ModelUtils.binary_preds_supervised(dnn, X_benign_dnn)\n",
    "preds_benign_aut = ModelUtils.binary_preds_unsupervised(autoencoder, X_benign_aut)\n",
    "preds_anomalous_dnn = ModelUtils.binary_preds_supervised(dnn, X_anomalous_dnn)\n",
    "preds_anomalous_aut = ModelUtils.binary_preds_unsupervised(autoencoder, X_anomalous_aut)\n",
    "\n",
    "#retrained prediction on NON adveersarial samples\n",
    "preds_benign_dnn_retrained = ModelUtils.binary_preds_supervised(dnn_retrained, X_benign_dnn)\n",
    "preds_benign_aut_retrained = ModelUtils.binary_preds_unsupervised(autoencoder, X_benign_aut, 0.035)\n",
    "preds_anomalous_dnn_retrained = ModelUtils.binary_preds_supervised(dnn_retrained, X_anomalous_dnn)\n",
    "preds_anomalous_aut_retrained = ModelUtils.binary_preds_unsupervised(autoencoder, X_anomalous_aut, 0.035)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION ERROR ON BENIGN SAMPLES -> BASE TARGET SYSTEM:\n",
      "False anomalous: 5691 / 100000 - Rate 5.691%\n",
      "----------\n",
      "PREDICTION ERROR ON BENIGN SAMPLES -> TARGET SYSTEM WITH ADVERSARIAL RETRAINING:\n",
      "False anomalous: 8031 / 100000 - Rate 8.031%\n",
      "\n",
      "\n",
      "PREDICTION ERROR ON ANOMALOUS SAMPLES -> BASE TARGET SYSTEM:\n",
      "False benign: 1512 / 100000 - Rate 1.512%\n",
      "----------\n",
      "PREDICTION ERROR ON ANOMALOUS SAMPLES -> TARGET SYSTEM WITH ADVERSARIAL RETRAINING:\n",
      "False benign: 57 / 100000 - Rate 0.056999999999999995%\n"
     ]
    }
   ],
   "source": [
    "print('PREDICTION ERROR ON BENIGN SAMPLES -> BASE TARGET SYSTEM:')\n",
    "checkFalseAnomalous(preds_benign_dnn, preds_benign_aut)\n",
    "print('----------')\n",
    "print('PREDICTION ERROR ON BENIGN SAMPLES -> TARGET SYSTEM WITH ADVERSARIAL RETRAINING:')\n",
    "checkFalseAnomalous(preds_benign_dnn_retrained, preds_benign_aut_retrained)\n",
    "print('\\n')\n",
    "print('PREDICTION ERROR ON ANOMALOUS SAMPLES -> BASE TARGET SYSTEM:')\n",
    "checkFalseBenign(preds_anomalous_dnn, preds_anomalous_aut)\n",
    "print('----------')\n",
    "print('PREDICTION ERROR ON ANOMALOUS SAMPLES -> TARGET SYSTEM WITH ADVERSARIAL RETRAINING:')\n",
    "checkFalseBenign(preds_anomalous_dnn_retrained, preds_anomalous_aut_retrained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
